{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Generator for WGAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Critic for WGAN\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# WGAN class\n",
    "class WGAN:\n",
    "    def __init__(self, latent_dim, output_dim, critic_iterations=5, weight_clip=0.01, lr=5e-5):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = Generator(latent_dim, output_dim)\n",
    "        self.critic = Critic(output_dim)\n",
    "        self.critic_iterations = critic_iterations\n",
    "        self.weight_clip = weight_clip\n",
    "        self.g_optimizer = optim.RMSprop(self.generator.parameters(), lr=lr)\n",
    "        self.c_optimizer = optim.RMSprop(self.critic.parameters(), lr=lr)\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Critic\n",
    "        for _ in range(self.critic_iterations):\n",
    "            self.c_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, self.latent_dim)\n",
    "            fake_data = self.generator(z).detach()\n",
    "            c_real = self.critic(real_data).reshape(-1)\n",
    "            c_fake = self.critic(fake_data).reshape(-1)\n",
    "            \n",
    "            loss_critic = -(torch.mean(c_real) - torch.mean(c_fake))\n",
    "            loss_critic.backward()\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in self.critic.parameters():\n",
    "                p.data.clamp_(-self.weight_clip, self.weight_clip)\n",
    "\n",
    "        # Train Generator\n",
    "        self.g_optimizer.zero_grad()\n",
    "        z = torch.randn(batch_size, self.latent_dim)\n",
    "        fake_data = self.generator(z)\n",
    "        c_fake = self.critic(fake_data).reshape(-1)\n",
    "        loss_gen = -torch.mean(c_fake)\n",
    "        loss_gen.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return loss_critic.item(), loss_gen.item()\n",
    "\n",
    "# Define the Conditional Generator for CWGAN\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, output_dim):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), self.latent_dim)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Conditional Critic for CWGAN\n",
    "class ConditionalCritic(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ConditionalCritic, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Conditional WGAN class\n",
    "class ConditionalWGAN:\n",
    "    def __init__(self, latent_dim, num_classes, output_dim, critic_iterations=5, weight_clip=0.01, lr=5e-5):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = ConditionalGenerator(latent_dim, num_classes, output_dim)\n",
    "        self.critic = ConditionalCritic(output_dim, num_classes)\n",
    "        self.critic_iterations = critic_iterations\n",
    "        self.weight_clip = weight_clip\n",
    "        self.g_optimizer = optim.RMSprop(self.generator.parameters(), lr=lr)\n",
    "        self.c_optimizer = optim.RMSprop(self.critic.parameters(), lr=lr)\n",
    "\n",
    "    def train_step(self, real_data, labels):\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Critic\n",
    "        for _ in range(self.critic_iterations):\n",
    "            self.c_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, self.latent_dim)\n",
    "            fake_data = self.generator(z, labels).detach()\n",
    "            c_real = self.critic(real_data, labels).reshape(-1)\n",
    "            c_fake = self.critic(fake_data, labels).reshape(-1)\n",
    "            \n",
    "            loss_critic = -(torch.mean(c_real) - torch.mean(c_fake))\n",
    "            loss_critic.backward()\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in self.critic.parameters():\n",
    "                p.data.clamp_(-self.weight_clip, self.weight_clip)\n",
    "\n",
    "        # Train Generator\n",
    "        self.g_optimizer.zero_grad()\n",
    "        z = torch.randn(batch_size, self.latent_dim)\n",
    "        fake_data = self.generator(z, labels)\n",
    "        c_fake = self.critic(fake_data, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(c_fake)\n",
    "        loss_gen.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return loss_critic.item(), loss_gen.item()\n",
    "\n",
    "# Training function for both GANs\n",
    "def train_gan(gan, dataloader, num_epochs, device, conditional=False):\n",
    "    gan.generator.to(device)\n",
    "    gan.critic.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            real_data = real_data.view(real_data.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            if conditional:\n",
    "                c_loss, g_loss = gan.train_step(real_data, labels)\n",
    "            else:\n",
    "                c_loss, g_loss = gan.train_step(real_data)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], c_loss: {c_loss:.4f}, g_loss: {g_loss:.4f}')\n",
    "\n",
    "# Function to generate samples\n",
    "def generate_samples(gan, num_samples, latent_dim, device, conditional=False, num_classes=10):\n",
    "    gan.generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        if conditional:\n",
    "            labels = torch.arange(10).repeat(num_samples // 10 + 1)[:num_samples].to(device)\n",
    "            samples = gan.generator(z, labels)\n",
    "        else:\n",
    "            samples = gan.generator(z)\n",
    "    return samples.view(num_samples, 1, 28, 28)\n",
    "\n",
    "# Function to visualize samples\n",
    "def visualize_samples(samples, title):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    output_dim = 784  # 28x28\n",
    "    num_classes = 10\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    lr = 5e-5\n",
    "    critic_iterations = 5\n",
    "    weight_clip = 0.01\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    mnist = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train WGAN\n",
    "    print(\"Training WGAN...\")\n",
    "    wgan = WGAN(latent_dim, output_dim, critic_iterations, weight_clip, lr)\n",
    "    train_gan(wgan, dataloader, num_epochs, device)\n",
    "\n",
    "    # Generate and visualize samples from WGAN\n",
    "    wgan_samples = generate_samples(wgan, 100, latent_dim, device)\n",
    "    visualize_samples(wgan_samples, \"WGAN Generated Samples\")\n",
    "\n",
    "    # Train Conditional WGAN\n",
    "    print(\"Training Conditional WGAN...\")\n",
    "    cwgan = ConditionalWGAN(latent_dim, num_classes, output_dim, critic_iterations, weight_clip, lr)\n",
    "    train_gan(cwgan, dataloader, num_epochs, device, conditional=True)\n",
    "\n",
    "    # Generate and visualize samples from Conditional WGAN\n",
    "    cwgan_samples = generate_samples(cwgan, 100, latent_dim, device, conditional=True)\n",
    "    visualize_samples(cwgan_samples, \"Conditional WGAN Generated Samples\")\n",
    "\n",
    "    print(\"Training and visualization complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
