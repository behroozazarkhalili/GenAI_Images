{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Generator for VanillaGAN\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(256, input_dim=latent_dim),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(512),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(1024),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(output_dim, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator for VanillaGAN\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(1024, input_dim=input_dim),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(512),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(256),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# VanillaGAN class\n",
    "class VanillaGAN:\n",
    "    def __init__(self, latent_dim, output_dim, lr=0.0002, beta1=0.5):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = Generator(latent_dim, output_dim)\n",
    "        self.discriminator = Discriminator(output_dim)\n",
    "        self.g_optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta1)\n",
    "        self.d_optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta1)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = tf.random.normal((batch_size, self.latent_dim))\n",
    "            fake_data = self.generator(z, training=True)\n",
    "            d_real_output = self.discriminator(real_data, training=True)\n",
    "            d_fake_output = self.discriminator(fake_data, training=True)\n",
    "            d_real_loss = self.loss_fn(real_labels, d_real_output)\n",
    "            d_fake_loss = self.loss_fn(fake_labels, d_fake_output)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = tf.random.normal((batch_size, self.latent_dim))\n",
    "            fake_data = self.generator(z, training=True)\n",
    "            g_fake_output = self.discriminator(fake_data, training=True)\n",
    "            g_loss = self.loss_fn(real_labels, g_fake_output)\n",
    "\n",
    "        g_grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "# Define the Generator for Conditional VanillaGAN\n",
    "class ConditionalGenerator(keras.Model):\n",
    "    def __init__(self, latent_dim, num_classes, output_dim):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.label_emb = keras.layers.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(256, input_dim=latent_dim + num_classes),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(512),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(1024),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dense(output_dim, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, z, labels):\n",
    "        z = tf.reshape(z, (-1, self.latent_dim))\n",
    "        c = self.label_emb(labels)\n",
    "        x = tf.concat([z, c], axis=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Discriminator for Conditional VanillaGAN\n",
    "class ConditionalDiscriminator(keras.Model):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.label_emb = keras.layers.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(1024, input_dim=input_dim + num_classes),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(512),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(256),\n",
    "            keras.layers.LeakyReLU(0.2),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, labels):\n",
    "        x = tf.reshape(x, (-1, 784))\n",
    "        c = self.label_emb(labels)\n",
    "        x = tf.concat([x, c], axis=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Conditional VanillaGAN class\n",
    "class ConditionalVanillaGAN:\n",
    "    def __init__(self, latent_dim, num_classes, output_dim, lr=0.0002, beta1=0.5):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = ConditionalGenerator(latent_dim, num_classes, output_dim)\n",
    "        self.discriminator = ConditionalDiscriminator(output_dim, num_classes)\n",
    "        self.g_optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta1)\n",
    "        self.d_optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta1)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_data, labels):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = tf.random.normal((batch_size, self.latent_dim))\n",
    "            fake_data = self.generator(z, labels, training=True)\n",
    "            d_real_output = self.discriminator(real_data, labels, training=True)\n",
    "            d_fake_output = self.discriminator(fake_data, labels, training=True)\n",
    "            d_real_loss = self.loss_fn(real_labels, d_real_output)\n",
    "            d_fake_loss = self.loss_fn(fake_labels, d_fake_output)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = tf.random.normal((batch_size, self.latent_dim))\n",
    "            fake_data = self.generator(z, labels, training=True)\n",
    "            g_fake_output = self.discriminator(fake_data, labels, training=True)\n",
    "            g_loss = self.loss_fn(real_labels, g_fake_output)\n",
    "\n",
    "        g_grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "# Training function for both GANs\n",
    "def train_gan(gan, dataset, num_epochs, conditional=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataset):\n",
    "            real_data = tf.reshape(real_data, (-1, 784))\n",
    "            \n",
    "            if conditional:\n",
    "                d_loss, g_loss = gan.train_step(real_data, labels)\n",
    "            else:\n",
    "                d_loss, g_loss = gan.train_step(real_data)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], d_loss: {d_loss:.4f}, g_loss: {g_loss:.4f}')\n",
    "\n",
    "# Function to generate samples\n",
    "def generate_samples(gan, num_samples, latent_dim, conditional=False, num_classes=10):\n",
    "    z = tf.random.normal((num_samples, latent_dim))\n",
    "    if conditional:\n",
    "        labels = tf.repeat(tf.range(10), num_samples // 10 + 1)[:num_samples]\n",
    "        samples = gan.generator(z, labels, training=False)\n",
    "    else:\n",
    "        samples = gan.generator(z, training=False)\n",
    "    return tf.reshape(samples, (num_samples, 28, 28, 1))\n",
    "\n",
    "# Function to visualize samples\n",
    "def visualize_samples(samples, title):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i, :, :, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    output_dim = 784  # 28x28\n",
    "    num_classes = 10\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    lr = 0.0002\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(batch_size)\n",
    "\n",
    "    # Train VanillaGAN\n",
    "    print(\"Training VanillaGAN...\")\n",
    "    vanilla_gan = VanillaGAN(latent_dim, output_dim, lr, beta1)\n",
    "    train_gan(vanilla_gan, dataset, num_epochs)\n",
    "\n",
    "    # Generate and visualize samples from VanillaGAN\n",
    "    vanilla_samples = generate_samples(vanilla_gan, 100, latent_dim)\n",
    "    visualize_samples(vanilla_samples, \"VanillaGAN Generated Samples\")\n",
    "\n",
    "    # Train Conditional VanillaGAN\n",
    "    print(\"Training Conditional VanillaGAN...\")\n",
    "    cond_vanilla_gan = ConditionalVanillaGAN(latent_dim, num_classes, output_dim, lr, beta1)\n",
    "    train_gan(cond_vanilla_gan, dataset, num_epochs, conditional=True)\n",
    "\n",
    "    # Generate and visualize samples from Conditional VanillaGAN\n",
    "    cond_vanilla_samples = generate_samples(cond_vanilla_gan, 100, latent_dim, conditional=True)\n",
    "    visualize_samples(cond_vanilla_samples, \"Conditional VanillaGAN Generated Samples\")\n",
    "\n",
    "    print(\"Training and visualization complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
